---
title: "Econometrics II: Assignment 2"
author: "Walter Verwer & Bas Machielsen"
date: \today
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE)

if(Sys.info()["user"] == "bas"){ # Mijn Linux
    Sys.setenv(RETICULATE_PYTHON = "/home/bas/anaconda3/bin/python")
    library(reticulate)
    reticulate::use_python("/home/bas/anaconda3/bin/python")
    py_available(TRUE)
} else if(Sys.info()["user"] == "basmachielsen"){# Mijn Mac
    Sys.setenv(RETICULATE_PYTHON = "/opt/anaconda3/bin/python3")
    library(reticulate)
    reticulate::use_python("/opt/anaconda3/bin/python3")
    py_available(TRUE)
} else {
    Sys.setenv(RETICULATE_PYTHON = "C:\\Users\\walte\\anaconda3")
    library(reticulate)
    reticulate::use_python("C:\\Users\\walte\\anaconda3")
    py_available(TRUE)
}

# Hier kunnen we zien of alles is goed gegaan 
py_config()
```

## Question 1

First use pooled OLS to check the impact of including and excluding ASVABC on the estimate of $\alpha_1$. Present and explain the result.

```{python load packages, echo=FALSE}
import pandas as pd
import getpass


import statsmodels.api as sm
from statsmodels.iolib.summary2 import summary_col
from stargazer.stargazer import Stargazer

from IPython.core.display import HTML
import numpy as np
```


```{python read data, echo=FALSE}
path_walter = ("D:\\Files\\Git Projects\\ti_git_projects\\year_1\\block_3\\") \
                + ("econometrics2\\assignment2\\data\\NLSY2000RC_V2.csv")

if getpass.getuser()=='walte':
    earnings = pd.read_csv(path_walter)
    
else: # voor bas:
    earnings = pd.read_csv("~/Documents/git/econometrics2/assignment2/data/NLSY2000RC_V2.csv")

# Log earnings:
earnings['EARNINGS'] = np.log(earnings['EARNINGS'])

ivs = ['ASVABC','AGE', 'AGESQ','S','ETHBLACK','URBAN','REGNE','REGNC','REGW','REGS']
y = ['EARNINGS']

earnings = earnings.dropna()
```

```{python results='hide'}
model_with = sm.OLS(endog=earnings[y], exog=sm.add_constant(earnings[ivs])).fit()

model_without = sm.OLS(endog=earnings[y], exog=sm.add_constant(earnings[ivs[1:]])).fit()

stargazer = Stargazer([model_with,  model_without])
stargazer.covariate_order(ivs)

with open("table1.tex", "w") as f:
    f.write(stargazer.render_latex())
```

\input{table1.tex}

The inclusion of the proxy for ability decreases the estimate for the coefficient of schooling. Hence, given all other standard assumptions, ability and schooling are positively correlated, and the omission of a proxy for ability overestimates the impact of schooling. Something else that we are able to observe is that because of the inclusion of the test scores, the adjusted-$R^2$ increases by approximately 0.2 points. This implies that including test scores add about 2% to the model's ability to explain the observed variation. Finally, we could do a very rough t-test to compare both estimates of schooling. We observe a standard-error of about 0.001, and we observe a change in the coefficients of roughly 0.022. This implies that the t-value of the corresponding t-test is roughly 22. What this implies is that the observed difference in the estimate of the coefficient is significantly different.  

\clearpage

## Question 2

Perform a pooled OLS analysis to obtain insight in the heterogeneity of returns to schooling by ethnicity. Present the results and comment on the outcomes: what are the conclusions based on this?

```{python results='hide'}
# including a cross effect of schooling and ethnicity
earnings['BLACKxS'] = earnings['ETHBLACK'] * earnings['S']

ivs2 = ivs + ['BLACKxS']

est_ear_inter = sm.OLS(endog=earnings[y], exog = sm.add_constant(earnings[ivs2])).fit()

# estimating separate equations by ethnicity
earnings_noblack = earnings[earnings['ETHBLACK'] == 0]
earnings_black = earnings[earnings['ETHBLACK'] == 1]


est_ear_noblack = (sm.OLS(
    endog=earnings_noblack[y],
    exog=sm.add_constant(earnings_noblack[ivs]))
    .fit()
    )

est_ear_black = (sm.OLS(
    endog=earnings_black[y],
    exog=sm.add_constant(earnings_black[ivs]))
    .fit()
    )


stargazer = Stargazer([est_ear_inter, est_ear_noblack,  est_ear_black])
stargazer.covariate_order(['BLACKxS']+ivs)

stargazer.show_f_statistic = False
stargazer.show_residual_std_err = False
stargazer.custom_columns(['Interaction','Not Black', 'Black'], [1,1,1])

with open("table2.tex", "w") as f:
    f.write(stargazer.render_latex())

```

\input{table2.tex}

We can see that the interaction effect is significant: that is to say, there is a significant difference between blacks and non-black in the influence of schooling on earnings. When we split up the sample into blacks and non-black, we get a similar view: the point estimate for the effect of schooling seems to be slightly lower for black people than for non-black people. As seen in the pooled regression with interaction effect, the differential impact is statistically significant. This follows from the fact that the change is roughly 0.015 and if we take the highest standard error for the two (0.003), we would obtain a t-statistic of about 5. Which means that the observed difference is very likely to be because of ethnicity. Interestingly, the interaction approach results a very similar estimate and standard error as the difference between the two models. This implies that both methods give similar results. To conclude, we observe that there is a racial difference in the influence of schooling on earnings for blacks and non-blacks. It appears that blacks benefit more from schooling than whites, in terms of earnings.

\clearpage

## Question 3

Perform the analysis for heterogenous schooling effects using the random effects model. Present the results and compare the outcomes with the pooled OLS results obtained before. Interpret the outcomes.

<!-- Ik schat hier Random effects zonder intercept, zodat alle 4 de regio dummies erin kunnen. Ik heb dit ook in Python gedaan, en de resultaten zijn 100% identiek. Het probleem is alleen dat het super moeilijk is om van python linearmodels te exporteren naar .tex... echt een drama. Daarom heb ik maar plm en R gebruikt. 

W: Dit vind ik ook netter :)-->

```{r, include = TRUE, echo = FALSE, results='asis'}
library(plm)
library(stargazer)

formula <- paste0(py$y, " ~ ", paste(py$ivs2, collapse = " + "), " + 0")

random_effects <- plm(formula = formula, 
                      data = py$earnings, 
                      index = c("ID","TIME"),
                      model = "random")

stargazer(random_effects, 
          header=FALSE,
          title='Random effects model')
```

```{python, eval = FALSE, include=FALSE}
# Hier is dus de Python code, run maar, en je kunt zien dat er hetzelfde uitkomt
from linearmodels import RandomEffects
import texression
from linearmodels import OLS

earnings_indexed = earnings.set_index(['ID', 'TIME'])
#earnings['TIME'] = time

hoi = earnings_indexed[ivs]
dep = earnings_indexed[y]

re_model = RandomEffects(dep, hoi).fit()

print(re_model.summary)
```

The point estimate for schooling is now close to the point estimate for schooling in the pooled OLS regression including the proxy for ability. Hence, the random effects estimator looks a lot like the pooled estimator, indicating that the contribution from the within group estimator is marginal. This can also be observed when looking at the decomposition of the explained variance: the between R-squared is larger than the within R-squared, indicating the model does a better job explaining the changes between individuals rather than individuals over time. 

(Still need to explain: black ethnicity $\cdot$ schooling is significant)

\clearpage

## Question 4

__A priori, would you plead for using fixed effects estimation or random effects estimation? Explain your answer.__

A priori, it would make more sense to use fixed-effects rather than random effects, because it is very likely that the unobservable individual components $\eta_i$ are correlated to the predictor variables $X$ rather than being random. For example, $\eta_i$ can be interpreted as being some measure of ability or innate willingness to exert effort, and that is likely related to age, schooling and test score. A possible correlation would violate the randomness of $\eta_i$ required by random effects, and hence, fixed effects would be preferred. 

\clearpage

## Question 5

__Apply the fixed effects estimator to analyze the heterogenous schooling effect. Interpret the outcomes.__

```{r fixedeffects, include = TRUE, echo = FALSE, results='asis'}
library(plm)
library(stargazer)

formula <- paste0(py$y, " ~ ", paste(py$ivs2, collapse = " + "))

fixed_effects <- plm(formula = formula, 
                      data = py$earnings, 
                      index = c("ID","TIME"),
                      model = "within")

stargazer(fixed_effects, 
          header=FALSE)
```

\clearpage
## Question 6

Fixed effects estimation may not be as efficient as random effects estimation, but is robust to correlation between regressors and the random effect. Can we perform a Hausman test in this context? Perform the test you propose.

The test tests the null hypothesis that the unique errors are not correlated with the regressors. 

```{r hausman, include=TRUE}
phtest(fixed_effects, random_effects)
```

The null hypothesis is rejected, implying that the unique parts are correlated with the regressors, and hence, random effects is an inconsistent estimator. 

## Question 7

For this question we first need to estimate the time mean of the regressor variables, for every individual.

```{r mundlak, include=TRUE}
# 1. Estimate time means per individual and variable:
    # We moeten mergen, de mundlak part moet constant erbij toegevoegd worden, for elke t.
    # Dus voor elke individu neem de mean over de tijd en maak de dimensies gelijk aan Y_it

# 2. Use pggls() to estimate the feasible GLS of the model, use method = random:
    # Idem

# 3. Apply Wald test 

# wald.test(vcov(ppgls(model)), b=coeffs(ppgls(models)), Terms = 10:17, df = earnings)

library(aod)

# get average over time per worker
X_hat <- py$earnings %>%
  group_by(ID) %>%
  summarize(across(c(AGE, AGESQ, ETHBLACK, 
                     URBAN, ASVABC, REGNE, 
                     REGNC, REGW, 
                     REGS, BLACKxS),
                   ~ mean(.)
                   )
            )

colnames(X_hat)[c(-1)] <- paste(colnames(X_hat)[c(-1)], "MEAN", sep = "_")

# add to individual variables
earnings_with_mean <- merge(py$earnings, X_hat, by = "ID")

mundlak <- pggls(EARNINGS ~  S + AGE + AGESQ + ETHBLACK + URBAN + REGNE + REGNC + REGW  + ASVABC + S_MEAN + AGE_MEAN + AGESQ_MEAN + ETHBLACK_MEAN + URBAN_MEAN + REGNE_MEAN + REGNC_MEAN + REGW_MEAN  + ASVABC_MEAN, data=earnings_with_mean, model="random",index = c("ID"))

summary(mundlak)
stargazer(mundlak, header=FALSE)

wald.test(vcov(mundlak), b=coef(mundlak), Terms = 10:17)

```


## Question 9

```{r verbeek_neijman, include=TRUE}
# 1. construct d_i (1 if in there for 5 waves):

# library(plyr)
# # count number of times an id occurs
# freq <- count(py$earnings,c('ID'))
# 
# earnings <- py$earnings
# 
# earnings$dummy <- ifelse(freq>=5 , 1, nan) # if frequancy >=, then d_unem is 1. Else nan.

```









